# -*- coding: utf-8 -*-
"""11/23_Boy_TL for 8leaf_DenseNet121_input=200.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nQDUeVIDFLsl4OMarDkXJkbC4VV2b0NA
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import cv2
import os
import glob
import pandas as pd
import matplotlib.pyplot as plt

TRAIN_PATH = ["/content/drive/My Drive/Colab Notebooks//"]
TRAIN_CAT = ['AE', 'DR', 'FF', 'FP', 'LF', 'PF', 'PP', 'TC']

TEST_PATH = ["/content/drive/My Drive/Colab Notebooks//"]
TEST_CAT = ['AE', 'DR', 'FF', 'FP', 'LF', 'PF', 'PP', 'TC']

RES = 400
COLOR_MONO = {'cv2_color': cv2.IMREAD_GRAYSCALE, 'cv2_interpolation': cv2.INTER_LANCZOS4, 'keras_channel': 1}
COLOR_BGR = {'cv2_color': cv2.IMREAD_COLOR, 'cv2_interpolation': cv2.INTER_LANCZOS4, 'keras_channel': 3}
COLOR_PARAM = COLOR_BGR

# prepare data, return data list and lable list
# nums: 0 => 全部、
def read_img(cats, paths, nums): # Data preparation
    X = []
    Y = []
    for i,c in enumerate(cats):
        ff = []
        lab = np.zeros(len(cats), dtype=float)
        lab[i] = 1
        n = 0
        for p in paths:
            fs = glob.glob(p + '/' + c + '/*.*') # image files may ended with .jpg or .JPG
            ff.extend(fs)
        ff = sorted(ff, key=os.path.getmtime, reverse=True)
        for f in ff:
            img_bgr = cv2.imread(f, COLOR_PARAM['cv2_color'])
            b,g,r = cv2.split(img_bgr)
            img = cv2.merge([r,g,b])
            img_resz = cv2.resize(img, (RES,RES), interpolation=COLOR_PARAM['cv2_interpolation'])
            img_reshape = np.reshape(img_resz, (RES,RES,COLOR_PARAM['keras_channel'])) /255.

            X.append(img_reshape)
            Y.append(lab)

            n += 1
            if nums != 0 and n == nums:
              break

    return X, Y

#colab 分配給我的 GPU 類型? #A80就重跑，P100才好
!nvidia-smi

from sklearn.utils import shuffle
from time import time, ctime

start = time()
x_tr, y_tr = read_img(TRAIN_CAT, TRAIN_PATH, 200)
x_train, y_train = shuffle(np.array(x_tr), np.array(y_tr), random_state=42)

x_te, y_te = read_img(TEST_CAT, TEST_PATH, 0)
x_test, y_test = shuffle(np.array(x_te), np.array(y_te), random_state=42)

print(x_train.shape)
print(x_test.shape)
print(f'training data loaded for {time()-start:.2f} secs')

# from sklearn.model_selection import train_test_split
# x_train, x_test, y_train, y_test = train_test_split(X_TR, Y_TR, test_size=0.1, random_state=42)

plt.imshow(x_train[14])

from keras.preprocessing.image import ImageDataGenerator
# augmentation
datagen = ImageDataGenerator(
     rotation_range=180,  # randomly rotate images in the range (degrees, 0 to 180)
     width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)
     height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)
     #channel_shift_range=20,  # change color
     horizontal_flip=True,  # randomly flip images
     #vertical_flip=True,  # randomly flip images
     #zoom_range=0.2
     #fill_mode='constant'
)

"""**Model Training**"""

from keras.applications.densenet import DenseNet121
mob = DenseNet121(include_top=False, input_shape=(RES,RES,3), weights='imagenet')
# mob.summary()


from keras.layers import Flatten, Dense, Dropout, GlobalAveragePooling2D
from keras.models import Model
from keras.layers import BatchNormalization

# 固定tl model的layers不要訓練
for l in mob.layers:
    l.trainable = True
# for l in mob.layers[:30]:
#     l.trainable=False
# for l in mob.layers[30:]:
#     l.trainable=True

# 調整model的輸出去適應mlp，避免relu死去
x = BatchNormalization(epsilon=1e-3,momentum=0.999)(mob.output)
x = GlobalAveragePooling2D()(x)
# x = Flatten()(x)
x = Dense(1024, activation="relu")(x)
x = Dropout(0.5)(x)
x = Dense(256, activation="relu")(x)
x = Dropout(0.5)(x)
x = Dense(8, activation="softmax")(x)
cnn = Model(inputs=mob.input, outputs=x)
cnn.summary()

cnn.compile(loss="categorical_crossentropy",
           optimizer="adam",
           metrics=["accuracy"])

#載入舊有模型
# from keras.models import load_model
# cnn = load_model('/content/drive/My Drive/Colab Notebooks/Tree/model/InceptionV3/14Tree/ep035-val_loss1.392-val_acc0.707.h5')

from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau
epochs = 300
batch_size = 25
steps_per_epoch = 40 # len(x_train) / batch_size

checkpoint = ModelCheckpoint('/content/drive/My Drive/Colab Notebooks////model_safe_zone/ep{epoch:03d}-val_loss{val_loss:.3f}-val_acc{val_acc:.3f}.h5', 
                             monitor='val_acc', save_weights_only=False, save_best_only=True, period=1, verbose=0)

train_history = cnn.fit_generator(
                  datagen.flow(x_train, y_train, batch_size=batch_size),
                  # class_weight=[1,1,1,1,1,1,1.2,1.2,1.2,1.2,1,1,1.2,1],
                  steps_per_epoch=steps_per_epoch,
                  epochs=epochs,
                  callbacks=[checkpoint],
                  validation_data=(x_test, y_test))

cnn.evaluate(x_test,y_test)
